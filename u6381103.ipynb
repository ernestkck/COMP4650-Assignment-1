{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Information Retrieval (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is based on the Assignment 1 Demo tutorials.\n",
    "\n",
    "In this assignment, your task is to index a new document collection into *Elasticsearch*, and then measure search performance based on predefined queries.\n",
    "\n",
    "A new document collection containing more than 10,000 goverment sites description, and a set of predefined queries, is provided for this assignment.\n",
    "\n",
    "Throughout this assginment: \n",
    "1. You will develop a better understanding of indexing, including the tokeniser, parser, and normaliser components, and how to improve the search performance given a predefined evaluation metric, \n",
    "2. You will develop a better understanding of search algorithms, and how to obtain better search results, and \n",
    "3. You will find the best way to combine an indexer and search algorithm to maximise your performance.\n",
    "\n",
    "Below, you will solve five programming questions, and three written questions. \n",
    "\n",
    "We will check the correctness of your code and the overall performance score.\n",
    "\n",
    "- Write your code after `### Your code here`, and remove `raise NotImplementedError` after implementation.\n",
    "- Write answers in this notebook file in the designated cells, and upload the file to the Wattle submission site. **Please rename and submit jupyter notebook file (`Assignment1.ipynb`) to `your_uid.ipynb` (e.g. `u1234567.ipynb`) with your written answers therein - FAILURE TO DO SO WILL RESULT IN -0.5 POINTS**. \n",
    "\n",
    "*Hint*: After finishing coding your notebook, select from the Jupyter Notebook interface the menu option Kernel -> Restart & Run All. After the execution of each block is finished, inspect the output, save the notebook and shutdown the kernel. Only now you can safely manipulate the .ipynb file, which contains code, explanations and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding component (Q1 - Q5), 4 marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Index Gov dataset (0.5 marks)\n",
    "\n",
    "For this assignment we will be working with a corpus of government documents, located in the gov folder. \n",
    "\n",
    "The gov folder contains three sub-folders; documents, qrels and topics. The documents folder consists of sub-folders, each of which contain multiple documents. Topics and qrels contain search queries and corresponding ground truth relevant documents, respectively.\n",
    "\n",
    "Your first job is to index the documents as we have done in the tutorial exercises (Assignment 1 Demo).\n",
    "\n",
    "Note that depending on your machine, indexing may take several minutes to a few hours. You may implement multi-threaded version of indexing to mitigate this problem.\n",
    "\n",
    "Below is provided the basic code configuration for indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic configuration for indexing\n",
    "basic_settings = {\n",
    "  \"mappings\": {\n",
    "    \"doc\": {\n",
    "      \"properties\": {\n",
    "        \"filename\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"path\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"text\": {\n",
    "          \"type\": \"text\",\n",
    "          \"similarity\": \"boolean\",\n",
    "          \"analyzer\": \"my_analyzer\",\n",
    "          \"search_analyzer\": \"my_analyzer\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_analyzer\": {\n",
    "          \"filter\": [\n",
    "            \"stop\"\n",
    "          ],\n",
    "          \"char_filter\": [\n",
    "            \"html_strip\"\n",
    "          ],\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"whitespace\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the below function `build_gov_index`. Don't forget to remove `raise NotImplementedError` after implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "#  A document class with following attributes:\n",
    "#   filename: document filename\n",
    "#   text: body of documment\n",
    "#   path: path of document\n",
    "\n",
    "Doc = namedtuple('Doc', 'filename path text')\n",
    "\n",
    "def read_doc(doc_path, encoding):\n",
    "    '''\n",
    "        reads a document from path\n",
    "        input:\n",
    "            - doc_path : path of document\n",
    "            - encoding: encoding\n",
    "        output: =>\n",
    "            - doc: instance of Doc namedtuple\n",
    "    '''\n",
    "    filename = doc_path.split('/')[-1]\n",
    "    fp = open(doc_path, 'r', encoding = encoding)\n",
    "    text = fp.read().strip()\n",
    "    fp.close()\n",
    "    return Doc(filename = filename, text = text, path = doc_path)\n",
    "\n",
    "def read_dataset(path, encoding = \"ISO-8859-1\"):\n",
    "    '''\n",
    "        reads multiple documents from path\n",
    "        input:\n",
    "            - doc_path : path of document\n",
    "            - encoding: encoding\n",
    "        output: =>\n",
    "            - docs: instances of Doc namedtuple returned as generator\n",
    "    '''\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for doc_path in files:\n",
    "            yield read_doc(root + '/' + doc_path, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "ES_HOSTS = ['http://localhost:9200']\n",
    "DOCS_PATH = 'gov/documents'\n",
    "INDEX_NAME = 'gov'\n",
    "DOC_TYPE = 'doc'\n",
    "\n",
    "def build_gov_index(es_conn, index_name, doc_path, settings):\n",
    "    # TODO implement function that:\n",
    "    #  1. Create an index with `index_name`. If `index_name` already exists, remove the index first.\n",
    "    #  2. Index the documents under doc_path, including subfolders, into elasticsearch (hint: read demo carefully)\n",
    "    # Note that this function will be used throughout this assignment    \n",
    "    \n",
    "    # create the index if it doesn't exist\n",
    "    if es_conn.indices.exists(index_name):\n",
    "        es_conn.indices.delete(index = index_name)\n",
    "        print('index `{}` deleted'.format(index_name))\n",
    "    es_conn.indices.create(index = index_name, ignore = 400, body = settings)\n",
    "    print('index `{}` created'.format(index_name))     \n",
    "    \n",
    "    counter_idx_failed = 0, 0 # counters\n",
    "\n",
    "    # retrive & index documents\n",
    "    def gendata():\n",
    "        counter_read = 0\n",
    "        for doc in dataset:\n",
    "            yield {\n",
    "                \"_index\": index_name,\n",
    "                \"_type\": DOC_TYPE,\n",
    "                \"_id\" : doc.filename,\n",
    "                \"_source\": doc._asdict(),\n",
    "            }\n",
    "            counter_read += 1\n",
    "        \n",
    "   \n",
    "    res = helpers.bulk(es_conn, gendata())\n",
    "    print (\"Indexed {} docs to index '{}'\".format(res[0], index_name))\n",
    "    \n",
    "    # refresh after indexing\n",
    "    es_conn.indices.refresh(index=index_name)  \n",
    "\n",
    "es_conn = Elasticsearch(ES_HOSTS)\n",
    "dataset = read_dataset(DOCS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index `gov` deleted\n",
      "index `gov` created\n",
      "Indexed 33848 docs to index 'gov'\n"
     ]
    }
   ],
   "source": [
    "build_gov_index(es_conn, INDEX_NAME, dataset, basic_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 33848,\n",
       " '_shards': {'total': 5, 'successful': 5, 'skipped': 0, 'failed': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_conn.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Search and performance measure (0.5 marks)\n",
    "\n",
    "For this second task, you will first need to read the topics/gov.topics file. \n",
    "\n",
    "As we have done in the demo tutorial, each file is formatted as `query_id query_terms`, where\n",
    "query_id is a numerical number, and query_terms consists of multiple keywords as search terms. \n",
    "\n",
    "Your job is to read the query file and search using the provided search function. You will need to write the output of the search results to an output.txt file in the trec-eval standard format used in the demo tutorial. \n",
    "\n",
    "As a reminder, this means that each result of each query should be put in a line in the output.txt like this:\n",
    "\n",
    "`01 Q0 email09 0 1.23 my_IR_system1`\n",
    "\n",
    "`01 Q0 email09 1 1.11 my_IR_system1`\n",
    "\n",
    "`02 Q0 email07 0 1.08 my_IR_system1`\n",
    "\n",
    "where '01' is the query ID; ignore 'Q0'; 'emailxx' is the name of the file; '0' (or '1' or some other integer number) is the rank of this result; '1.23' (or '1.08' or some other number) is the score of this result; and 'my_IR_system1' is the name for your retrieval system. \n",
    "\n",
    "Note that you are only allowed to write 10-documents at most for each query. If your output file contains more than 10 documents per query, you will get 0 score for this question.\n",
    "\n",
    "**Please rename your output_q2.txt file to `YourUID_output_q2.txt` eg `u1234567_output_q2.txt`, before submitting to Wattle - FAILURE TO DO SO WILL RESULT IN -0.5 POINTS**.\n",
    "\n",
    "Below is some code to get you started and for you to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_string, es_conn, index_name):\n",
    "    '''\n",
    "        searches for query_string with default search algorithm\n",
    "        input:\n",
    "            - query_string: a query\n",
    "            - es_conn: elasticsearch connection\n",
    "            - index_name: name of index\n",
    "        output:\n",
    "            - a generator of tuple (filename, score)\n",
    "\n",
    "    '''\n",
    "    res = es_conn.search(index = index_name,\n",
    "        body = {\n",
    "            \"_source\": [ \"filename\"],\n",
    "            \"query\": {\n",
    "                \"query_string\": {\n",
    "                    \"query\": query_string,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return res['hits']['hits']\n",
    "\n",
    "# TODO: \n",
    "#       Read query file from `query_path`, search using `search_fn`, and \n",
    "#       Write top 10 outputs per query to `output_file`\n",
    "#       Note that the function takes a search function as an argument. You can directly call the search function\n",
    "#       as `result = search_fn(query_string, es_conn, index_name)` within the function.\n",
    "#       This function will be used throughout this assignment\n",
    "def read_search_write_output(search_fn, query_path, output_file):\n",
    "    with open(query_path, 'r') as f:\n",
    "            query_strings = f.readlines()\n",
    "    \n",
    "    with open(output_file, 'w') as output:\n",
    "        for query_string in query_strings:\n",
    "            query_terms = query_string[query_string.index(' ')+1:]\n",
    "            query_terms = query_terms.rstrip()\n",
    "            query_terms = query_terms.replace(\"\\/\", \" OR \")\n",
    "            query_terms = query_terms.replace(\"/\\\\\", \" AND \")\n",
    "\n",
    "            matches = search_fn(query_terms, es_conn, INDEX_NAME)\n",
    "\n",
    "            if matches is not None:\n",
    "                matches = sorted(matches, key = lambda x: -x['_score'])\n",
    "                for i in range(10):\n",
    "                    if (i >= len(matches)):\n",
    "                        break\n",
    "                    output.write('{} Q0 {} {} {} my_IR_system1\\n'.format(\n",
    "                        query_string.split(' ')[0],\n",
    "                        matches[i]['_source']['filename'], # filename\n",
    "                        str(i),\n",
    "                        matches[i]['_score'], # score\n",
    "                    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = 'gov/topics/gov.topics'\n",
    "output_file = 'u6381103_output_q2.txt'\n",
    "read_search_write_output(search, query_path, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have written the results of your query to an output file, you can run trec-eval on your output file and the provided gov.qrel file to evaluate your system. Trec-eval provides many different measures of quality, but for the purposes of this assignment you will use precision@10 (p_10 in trec-eval output) to measure the performance of your systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\tmy_IR_system1\n",
      "num_q                 \tall\t31\n",
      "num_ret               \tall\t310\n",
      "num_rel               \tall\t190\n",
      "num_rel_ret           \tall\t22\n",
      "map                   \tall\t0.0484\n",
      "gm_map                \tall\t0.0004\n",
      "Rprec                 \tall\t0.0874\n",
      "bpref                 \tall\t0.1200\n",
      "recip_rank            \tall\t0.1936\n",
      "iprec_at_recall_0.00  \tall\t0.1969\n",
      "iprec_at_recall_0.10  \tall\t0.1646\n",
      "iprec_at_recall_0.20  \tall\t0.1184\n",
      "iprec_at_recall_0.30  \tall\t0.0703\n",
      "iprec_at_recall_0.40  \tall\t0.0419\n",
      "iprec_at_recall_0.50  \tall\t0.0323\n",
      "iprec_at_recall_0.60  \tall\t0.0000\n",
      "iprec_at_recall_0.70  \tall\t0.0000\n",
      "iprec_at_recall_0.80  \tall\t0.0000\n",
      "iprec_at_recall_0.90  \tall\t0.0000\n",
      "iprec_at_recall_1.00  \tall\t0.0000\n",
      "P_5                   \tall\t0.0774\n",
      "P_10                  \tall\t0.0710\n",
      "P_15                  \tall\t0.0473\n",
      "P_20                  \tall\t0.0355\n",
      "P_30                  \tall\t0.0237\n",
      "P_100                 \tall\t0.0071\n",
      "P_200                 \tall\t0.0035\n",
      "P_500                 \tall\t0.0014\n",
      "P_1000                \tall\t0.0007\n"
     ]
    }
   ],
   "source": [
    "!./trec_eval/trec_eval ./gov/qrels/gov.qrels u6381103_output_q2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Improving the search algorithm: compare similarity algorithms (1 mark)\n",
    "\n",
    "*Elasticsearch* also provides multiple configurable scoring algorithms. \n",
    "\n",
    "For this task, you will be asked to find a better similarity module to improve the search performance. Please refer [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/similarity.html) for a better understanding of the configurable elasticsearch similarity modules.\n",
    "\n",
    "Here's some code to get you started and for you to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define your own analyzer for indexing and searching\n",
    "q3_settings = {\n",
    "  \"mappings\": {\n",
    "    \"doc\": {\n",
    "      \"properties\": {\n",
    "        \"filename\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"path\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"similarity\": \"my_similarity\",\n",
    "            \"analyzer\": \"my_analyzer\",\n",
    "            \"search_analyzer\": \"my_analyzer\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"settings\": {\n",
    "    \"index\":{\n",
    "        \"similarity\" : {\n",
    "            \"my_similarity\" : {\n",
    "                \"type\" : \"BM25\",\n",
    "                \"k1\" : \"2.0\",\n",
    "                \"b\" : \"0.75\",\n",
    "                \"discount_overlaps\" : \"true\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_analyzer\": {\n",
    "          \"filter\": [\n",
    "            \"stop\"\n",
    "          ],\n",
    "          \"char_filter\": [\n",
    "            \"html_strip\"\n",
    "          ],\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"whitespace\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index `gov` deleted\n",
      "index `gov` created\n",
      "Indexed 33848 docs to index 'gov'\n"
     ]
    }
   ],
   "source": [
    "es_conn = Elasticsearch(ES_HOSTS, maxsize=25)\n",
    "dataset = read_dataset(DOCS_PATH)\n",
    "output_file = 'u6381103_output_q3.txt'\n",
    "\n",
    "build_gov_index(es_conn, INDEX_NAME, DOCS_PATH, q3_settings)\n",
    "read_search_write_output(search, query_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\tmy_IR_system1\n",
      "num_q                 \tall\t31\n",
      "num_ret               \tall\t310\n",
      "num_rel               \tall\t190\n",
      "num_rel_ret           \tall\t50\n",
      "map                   \tall\t0.1953\n",
      "gm_map                \tall\t0.0099\n",
      "Rprec                 \tall\t0.2226\n",
      "bpref                 \tall\t0.2922\n",
      "recip_rank            \tall\t0.4729\n",
      "iprec_at_recall_0.00  \tall\t0.4831\n",
      "iprec_at_recall_0.10  \tall\t0.4562\n",
      "iprec_at_recall_0.20  \tall\t0.4000\n",
      "iprec_at_recall_0.30  \tall\t0.3185\n",
      "iprec_at_recall_0.40  \tall\t0.2373\n",
      "iprec_at_recall_0.50  \tall\t0.1738\n",
      "iprec_at_recall_0.60  \tall\t0.1137\n",
      "iprec_at_recall_0.70  \tall\t0.0935\n",
      "iprec_at_recall_0.80  \tall\t0.0438\n",
      "iprec_at_recall_0.90  \tall\t0.0161\n",
      "iprec_at_recall_1.00  \tall\t0.0161\n",
      "P_5                   \tall\t0.2581\n",
      "P_10                  \tall\t0.1613\n",
      "P_15                  \tall\t0.1075\n",
      "P_20                  \tall\t0.0806\n",
      "P_30                  \tall\t0.0538\n",
      "P_100                 \tall\t0.0161\n",
      "P_200                 \tall\t0.0081\n",
      "P_500                 \tall\t0.0032\n",
      "P_1000                \tall\t0.0016\n"
     ]
    }
   ],
   "source": [
    "!./trec_eval/trec_eval ./gov/qrels/gov.qrels u6381103_output_q3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gov': {'settings': {'index': {'number_of_shards': '5',\n",
       "    'provided_name': 'gov',\n",
       "    'similarity': {'my_similarity': {'discount_overlaps': 'true',\n",
       "      'b': '0.75',\n",
       "      'type': 'BM25',\n",
       "      'k1': '2.0'}},\n",
       "    'creation_date': '1566052602097',\n",
       "    'analysis': {'analyzer': {'my_analyzer': {'filter': ['stop'],\n",
       "       'char_filter': ['html_strip'],\n",
       "       'type': 'custom',\n",
       "       'tokenizer': 'whitespace'}}},\n",
       "    'number_of_replicas': '1',\n",
       "    'uuid': 'CQ3ktJHcQpuL5pRO_f9ITQ',\n",
       "    'version': {'created': '6030099'}}}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_conn.indices.get_settings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the final output to Wattle, but **please first rename output_file to YourUID_output_q3.txt eg u1234567_output_q3.txt - FAILURE TO DO SO WILL RESULT IN -0.5 POINTS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Improving the indexer: compare different ways of indexing (1 mark)\n",
    "\n",
    "For this part, you will be asked to change the configuration of indexer (`basic_settings`) to improve the search performance.\n",
    "\n",
    "Please look at the elastic search official document [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html) for better understanding of configuration and other options.\n",
    "\n",
    "Note that you can check how your tokeniser tokenises your input string via the `analyze_query` function provided in the demo code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: configure settings to define your own analyzer for indexing\n",
    "q4_settings = {\n",
    "  \"mappings\": {\n",
    "    \"doc\": {\n",
    "      \"properties\": {\n",
    "        \"filename\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"path\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"similarity\" : \"my_similarity\",\n",
    "            \"analyzer\": \"my_analyzer\",\n",
    "            \"search_analyzer\": \"my_analyzer\",            \n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"settings\": {\n",
    "    \"index\":{\n",
    "        \"similarity\" : {\n",
    "            \"my_similarity\" : {\n",
    "                \"type\" : \"BM25\",\n",
    "                \"k1\" : \"2.0\",\n",
    "                \"b\" : \"0.75\",\n",
    "                \"discount_overlaps\" : \"true\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"analysis\": {\n",
    "        \"filter\": {\n",
    "            \"english_stop\": {\n",
    "              \"type\":       \"stop\",\n",
    "              \"stopwords\":  \"_english_\" \n",
    "            },\n",
    "            \"english_stemmer\": {\n",
    "              \"type\":       \"stemmer\",\n",
    "              \"language\":   \"english\"\n",
    "            },\n",
    "            \"english_possessive_stemmer\": {\n",
    "              \"type\":       \"stemmer\",\n",
    "              \"language\":   \"possessive_english\"\n",
    "            },\n",
    "        },\n",
    "      \"analyzer\": {\n",
    "        \"my_analyzer\": {\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"standard\",\n",
    "          \"filter\": [\n",
    "              \"asciifolding\",\n",
    "              \"english_possessive_stemmer\",\n",
    "              \"lowercase\",\n",
    "              \"english_stop\",\n",
    "              \"english_stemmer\",\n",
    "              \"porter_stem\"\n",
    "          ],\n",
    "          \"char_filter\": [\n",
    "              \"html_strip\"\n",
    "          ]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index `gov` deleted\n",
      "index `gov` created\n",
      "Indexed 33848 docs to index 'gov'\n"
     ]
    }
   ],
   "source": [
    "es_conn = Elasticsearch(ES_HOSTS, maxsize=25)\n",
    "dataset = read_dataset(DOCS_PATH)\n",
    "output_file = 'u6381103_output_q4.txt'\n",
    "\n",
    "build_gov_index(es_conn, INDEX_NAME, DOCS_PATH, q4_settings)\n",
    "read_search_write_output(search, query_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\tmy_IR_system1\n",
      "num_q                 \tall\t31\n",
      "num_ret               \tall\t310\n",
      "num_rel               \tall\t190\n",
      "num_rel_ret           \tall\t108\n",
      "map                   \tall\t0.5052\n",
      "gm_map                \tall\t0.0890\n",
      "Rprec                 \tall\t0.5046\n",
      "bpref                 \tall\t0.5963\n",
      "recip_rank            \tall\t0.6530\n",
      "iprec_at_recall_0.00  \tall\t0.6818\n",
      "iprec_at_recall_0.10  \tall\t0.6778\n",
      "iprec_at_recall_0.20  \tall\t0.6630\n",
      "iprec_at_recall_0.30  \tall\t0.6401\n",
      "iprec_at_recall_0.40  \tall\t0.6146\n",
      "iprec_at_recall_0.50  \tall\t0.5895\n",
      "iprec_at_recall_0.60  \tall\t0.5111\n",
      "iprec_at_recall_0.70  \tall\t0.4616\n",
      "iprec_at_recall_0.80  \tall\t0.3814\n",
      "iprec_at_recall_0.90  \tall\t0.2427\n",
      "iprec_at_recall_1.00  \tall\t0.2427\n",
      "P_5                   \tall\t0.5097\n",
      "P_10                  \tall\t0.3484\n",
      "P_15                  \tall\t0.2323\n",
      "P_20                  \tall\t0.1742\n",
      "P_30                  \tall\t0.1161\n",
      "P_100                 \tall\t0.0348\n",
      "P_200                 \tall\t0.0174\n",
      "P_500                 \tall\t0.0070\n",
      "P_1000                \tall\t0.0035\n"
     ]
    }
   ],
   "source": [
    "!./trec_eval/trec_eval ./gov/qrels/gov.qrels u6381103_output_q4.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'let', b'see', b'how', b'analyz', b'anali', b'analyz', b'token', b'tokeni', b'sentenc', b'univ', b'univ', b'univ', b'univ', b'univ', b'univ', b'univalv', b'univalv', b'univ', b'univoc', b'univalv']\n"
     ]
    }
   ],
   "source": [
    "def analyze_query(text, es_conn, index_name):\n",
    "    '''\n",
    "        analyzes any text with my_analyzer defined in es_settings.json\n",
    "        input:\n",
    "            - text: a query text\n",
    "            - es_conn: elasticsearch connection\n",
    "            - index_name: name of index\n",
    "        output:\n",
    "            - a list of tokens\n",
    "    '''\n",
    "\n",
    "    tokens = es_conn.indices.analyze(\n",
    "        index = index_name,\n",
    "        body = {\"text\": text, \"analyzer\": \"my_analyzer\"})['tokens']\n",
    "\n",
    "    return [token_row[\"token\"].encode('utf-8') for token_row in tokens]\n",
    "\n",
    "print(analyze_query(\n",
    "    \"Let's see. how the analyzer analyse analyze tokenize tokenise. this sentence.!\\\n",
    "     Universal university in the universe is universal universal univalent univalved univalves universes univocals univalve\", es_conn, INDEX_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the final output to Wattle, but **please first rename output_file to YourUID_output_q4.txt eg u1234567_output_q3.txt - FAILURE TO DO SO WILL RESULT IN -0.5 POINTS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Tolerant retrieval: wildcard queries (1 mark)\n",
    "\n",
    "*Elasticsearch* provides wildcard query search. You can use wildcard expressions cosisting of '*' and '?' to search.  \n",
    "\n",
    "For this task, you can reuse the previous index, i.e., *q4_settings*. Refer to the [link](https://www.elastic.co/guide/en/elasticsearch/reference/6.3/query-dsl-wildcard-query.html) to see how to search with wildcard queries. \n",
    "\n",
    "For each query term from 'gov.topics', replace last two characters with any wildcard expression. For example, the first topic from 'gov.topics' is 'mining gold silver coal'. Instead, you search 'mini&ast; go?? silv&ast; co??'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_search(query_string, es_conn, index_name):\n",
    "    wildcard_query = query_string.split(' ')\n",
    "    for i in range(len(wildcard_query)):\n",
    "        wildcard_query[i] = wildcard_query[i][:-2] + \"*\"\n",
    "    query_string = str.join(\" \", wildcard_query)\n",
    "    \n",
    "    res = es_conn.search(index = index_name,\n",
    "        body = {\n",
    "            \"_source\": [\"filename\"],\n",
    "            \"query\": {\n",
    "                \"query_string\": {\n",
    "                    \"query\": query_string\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return res['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_conn = Elasticsearch(ES_HOSTS, maxsize=25)\n",
    "dataset = read_dataset(DOCS_PATH)\n",
    "output_file = 'u6381103_output_q5.txt'\n",
    "read_search_write_output(my_search, query_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runid                 \tall\tmy_IR_system1\n",
      "num_q                 \tall\t30\n",
      "num_ret               \tall\t300\n",
      "num_rel               \tall\t186\n",
      "num_rel_ret           \tall\t19\n",
      "map                   \tall\t0.0357\n",
      "gm_map                \tall\t0.0001\n",
      "Rprec                 \tall\t0.0644\n",
      "bpref                 \tall\t0.0892\n",
      "recip_rank            \tall\t0.1056\n",
      "iprec_at_recall_0.00  \tall\t0.1111\n",
      "iprec_at_recall_0.10  \tall\t0.0963\n",
      "iprec_at_recall_0.20  \tall\t0.0630\n",
      "iprec_at_recall_0.30  \tall\t0.0607\n",
      "iprec_at_recall_0.40  \tall\t0.0552\n",
      "iprec_at_recall_0.50  \tall\t0.0167\n",
      "iprec_at_recall_0.60  \tall\t0.0167\n",
      "iprec_at_recall_0.70  \tall\t0.0167\n",
      "iprec_at_recall_0.80  \tall\t0.0000\n",
      "iprec_at_recall_0.90  \tall\t0.0000\n",
      "iprec_at_recall_1.00  \tall\t0.0000\n",
      "P_5                   \tall\t0.0667\n",
      "P_10                  \tall\t0.0633\n",
      "P_15                  \tall\t0.0422\n",
      "P_20                  \tall\t0.0317\n",
      "P_30                  \tall\t0.0211\n",
      "P_100                 \tall\t0.0063\n",
      "P_200                 \tall\t0.0032\n",
      "P_500                 \tall\t0.0013\n",
      "P_1000                \tall\t0.0006\n"
     ]
    }
   ],
   "source": [
    "!./trec_eval/trec_eval ./gov/qrels/gov.qrels u6381103_output_q5.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written component (Q6 - Q9), 6 marks\n",
    "\n",
    "Answer the following questions based on your implementation of Questions 1-5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 (1.5 marks): What changs did you make to the search similarity to improve the performance of the system? Why do you think it improved the performance?\n",
    "\n",
    "(provide answers below using bullet points with 2~3 items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* At first, our system used the basic settings, which uses boolean similarity. It simply compares the terms and returns a score based on whether the query terms match or not. If a document contain the exact terms, its score is 1. If it doesn't, the score is 0. It does not consider anything else.\n",
    "\n",
    "* We change the similarity type to BM25, which is considered to be a state-of-the-art ranking function. It uses term frequency, inverse document frequency, and field-length normalisation, which will all greatly improve the performance of the system compared to boolean similarity. Term frequency measures the amount of times a word appears in a document because a document is usually more relevant if it contains many occurrences of that word. Inverse document frequency gives a higher value to uncommon words because words that occur a lot are usually less significant. Field-length normalisation considers shorter fields to have more weight than longer fields.\n",
    "\n",
    "* However, there are a few differences with the classic TF/IDF algorithm in Elasticsearch which makes it perform better. It has nonlinear term-frequency saturation, which means terms that appear 10 times in a document have almost the same impact as terms that appear 1000 times. I changed k1 to value higher than default, which will result in a slower saturation. Besides, the field-length normalisation also takes the average length of the field into account. \n",
    "\n",
    "\n",
    "Source: https://www.elastic.co/guide/en/elasticsearch/guide/2.x/pluggable-similarites.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 (1.5 marks): What changes did you make to the indexer to improve the performance of the system? Why do you think it improved the performance?\n",
    "\n",
    "(provide answer below using bullet points with 2~3 items (Check [this](https://sourceforge.net/p/jupiter/wiki/markdown_syntax/#md_ex_lists) if you are not familiar with markdown syntax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The tokeniser is changed to the standard one instead of whitespace. The original tokeniser simply splits text whenever there is a whitespace character. The standard tokeniser improves performance by splitting based on grammer.\n",
    "\n",
    "* Then, we added a number of token filters to modify the tokens to improve the performance. \"asciifolding\" tries to convert characters that are not in the first 127 ASCII characters into their ASCII equivalents, for example some other symbols or Unicode characters. We also convert all text into lower case.\n",
    "\n",
    "* In addition, we remove stop words and apply a few different stemmers. Stop words are the most common words such as \"the\" and \"is\" which are of very little importance. Stemming reduces words to their \"stem\" form so words of different forms that have similar meaning are reduced to the same \"stem\" form. This improves the performane because we want to search for documents that contain relevant terms as well, not just the exact terms.\n",
    "\n",
    "Source: https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 (1.5 marks): Apart from Precision@10, what other metrics can be used to measure the performance of the developed IR system for the government document collection? Provide two metrics and explain why they would be suited for this particular government IR system.\n",
    "\n",
    "(provide answers below using bullet points with 2~3 items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mean average precision (map): Precision is the measure of true positives over total positive results. This gives us a measure of how accurate our predictions are.\n",
    "\n",
    "* Reciprocal rank (recip_rank): The reciprocal rank of a query response is the multiplicative inverse of the rank of the  correct answer. Since our output is sorted by the score, the documents that are supposed to be the most relevant are on the top. Thus, higher ranked outputs that are not actually relevant should get a higher \"penalty\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 (1.5 marks): How do wildcard queries affect performance of the retrieval in terms of measures you answered for Q8? Also provide some situations when wildcard queries are useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use wildcard queries when when we are uncertain of the spelling of a query term. For example, we may not be certain of some unfamiliar words in another language. In addtion, wildcard queries are useful when the term has multiple variants. For example, \"analyse, analyze and analysis\" and words that end with -ise/-ize. Wildcard queries are also useful on words that are stemmed. We could just search for the stemmed form of a word and add a wildcard at the end to get the result we want, which will work regardless of whether stemming is actually performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Academic Misconduct Policy**: All submitted written work and code must be your own (except for any provided starter code, of course) – submitting work other than your own will lead to both a failure on the assignment and a referral of the case to the ANU academic misconduct review procedures: ANU Academic Misconduct Procedures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
