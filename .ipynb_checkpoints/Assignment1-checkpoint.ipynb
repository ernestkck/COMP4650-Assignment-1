{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Information Retrieval (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is based on the Assignment 1 Demo tutorials.\n",
    "\n",
    "In this assignment, your task is to index a new document collection into *Elasticsearch*, and then measure search performance based on predefined queries.\n",
    "\n",
    "A new document collection containing more than 10,000 goverment sites description, and a set of predefined queries, is provided for this assignment.\n",
    "\n",
    "Throughout this assginment: \n",
    "1. You will develop a better understanding of indexing, including the tokeniser, parser, and normaliser components, and how to improve the search performance given a predefined evaluation metric, \n",
    "2. You will develop a better understanding of search algorithms, and how to obtain better search results, and \n",
    "3. You will find the best way to combine an indexer and search algorithm to maximise your performance.\n",
    "\n",
    "Below, you will solve five programming questions, and three written questions. \n",
    "\n",
    "We will check the correctness of your code and the overall performance score.\n",
    "\n",
    "- Write your code after `### Your code here`, and remove `raise NotImplementedError` after implementation.\n",
    "- Write answers in this notebook file in the designated cells, and upload the file to the Wattle submission site. **Please rename and submit jupyter notebook file (`Assignment1.ipynb`) to `your_uid.ipynb` (e.g. `u1234567.ipynb`) with your written answers therein - FAILURE TO DO SO WILL RESULT IN -0.5 POINTS**. \n",
    "\n",
    "*Hint*: After finishing coding your notebook, select from the Jupyter Notebook interface the menu option Kernel -> Restart & Run All. After the execution of each block is finished, inspect the output, save the notebook and shutdown the kernel. Only now you can safely manipulate the .ipynb file, which contains code, explanations and output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding component (Q1 - Q5), 4 marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: Index Gov dataset (0.5 marks)\n",
    "\n",
    "For this assignment we will be working with a corpus of government documents, located in the gov folder. \n",
    "\n",
    "The gov folder contains three sub-folders; documents, qrels and topics. The documents folder consists of sub-folders, each of which contain multiple documents. Topics and qrels contain search queries and corresponding ground truth relevant documents, respectively.\n",
    "\n",
    "Your first job is to index the documents as we have done in the tutorial exercises (Assignment 1 Demo).\n",
    "\n",
    "Note that depending on your machine, indexing may take several minutes to a few hours. You may implement multi-threaded version of indexing to mitigate this problem.\n",
    "\n",
    "Below is provided the basic code configuration for indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic configuration for indexing\n",
    "basic_settings = {\n",
    "  \"mappings\": {\n",
    "    \"doc\": {\n",
    "      \"properties\": {\n",
    "        \"filename\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"path\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"text\": {\n",
    "          \"type\": \"text\",\n",
    "          \"similarity\": \"boolean\",\n",
    "          \"analyzer\": \"my_analyzer\",\n",
    "          \"search_analyzer\": \"my_analyzer\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_analyzer\": {\n",
    "          \"filter\": [\n",
    "            \"stop\"\n",
    "          ],\n",
    "          \"char_filter\": [\n",
    "            \"html_strip\"\n",
    "          ],\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"whitespace\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to implement the below function `build_gov_index`. Don't forget to remove `raise NotImplementedError` after implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "ES_HOSTS = ['http://localhost:9200']\n",
    "DOCS_PATH = 'gov/documents'\n",
    "INDEX_NAME = 'gov'\n",
    "DOC_TYPE = 'doc'\n",
    "\n",
    "def build_gov_index(es_conn, index_name, doc_path, settings):\n",
    "    # TODO implement function that:\n",
    "    #  1. Create an index with `index_name`. If `index_name` already exists, remove the index first.\n",
    "    #  2. Index the documents under doc_path, including subfolders, into elasticsearch (hint: read demo carefully)\n",
    "    # Note that this function will be used throughout this assignment    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "es_conn = Elasticsearch(ES_HOSTS)\n",
    "build_gov_index(es_conn, INDEX_NAME, DOCS_PATH, basic_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: Search and performance measure (0.5 marks)\n",
    "\n",
    "For this second task, you will first need to read the topics/gov.topics file. \n",
    "\n",
    "As we have done in the demo tutorial, each file is formatted as `query_id query_terms`, where\n",
    "query_id is a numerical number, and query_terms consists of multiple keywords as search terms. \n",
    "\n",
    "Your job is to read the query file and search using the provided search function. You will need to write the output of the search results to an output.txt file in the trec-eval standard format used in the demo tutorial. \n",
    "\n",
    "As a reminder, this means that each result of each query should be put in a line in the output.txt like this:\n",
    "\n",
    "`01 Q0 email09 0 1.23 my_IR_system1`\n",
    "\n",
    "`01 Q0 email09 1 1.11 my_IR_system1`\n",
    "\n",
    "`02 Q0 email07 0 1.08 my_IR_system1`\n",
    "\n",
    "where '01' is the query ID; ignore 'Q0'; 'emailxx' is the name of the file; '0' (or '1' or some other integer number) is the rank of this result; '1.23' (or '1.08' or some other number) is the score of this result; and 'my_IR_system1' is the name for your retrieval system. \n",
    "\n",
    "Note that you are only allowed to write 10-documents at most for each query. If your output file contains more than 10 documents per query, you will get 0 score for this question.\n",
    "\n",
    "**Please rename your output_q2.txt file to `YourUID_output_q2.txt` eg `u1234567_output_q2.txt`, before submitting to Wattle - FAILURE TO DO SO WILL RESULT IN -0.5 POINTS**.\n",
    "\n",
    "Below is some code to get you started and for you to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query_string, es_conn, index_name):\n",
    "    '''\n",
    "        searches for query_string with default search algorithm\n",
    "        input:\n",
    "            - query_string: a query\n",
    "            - es_conn: elasticsearch connection\n",
    "            - index_name: name of index\n",
    "        output:\n",
    "            - a generator of tuple (filename, score)\n",
    "\n",
    "    '''\n",
    "    res = es_conn.search(index = index_name,\n",
    "        body = {\n",
    "            \"_source\": [ \"filename\"],\n",
    "            \"query\": {\n",
    "                \"query_string\": {\n",
    "                    \"query\": query_string,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return res['hits']['hits']\n",
    "\n",
    "# TODO: \n",
    "#       Read query file from `query_path`, search using `search_fn`, and \n",
    "#       Write top 10 outputs per query to `output_file`\n",
    "#       Note that the function takes a search function as an argument. You can directly call the search function\n",
    "#       as `result = search_fn(query_string, es_conn, index_name)` within the function.\n",
    "#       This function will be used throughout this assignment\n",
    "def read_search_write_output(search_fn, query_path, output_file):\n",
    "    with open(output_file, 'w') as output:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "query_path = 'gov/topics/gov.topics'\n",
    "output_file = 'output.txt'\n",
    "read_search_write_output(search, query_path, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have written the results of your query to an output file, you can run trec-eval on your output file and the provided gov.qrel file to evaluate your system. Trec-eval provides many different measures of quality, but for the purposes of this assignment you will use precision@10 (p_10 in trec-eval output) to measure the performance of your systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./trec_eval/trec_eval ./gov/qrels/gov.qrels output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Improving the search algorithm: compare similarity algorithms (1 mark)\n",
    "\n",
    "*Elasticsearch* also provides multiple configurable scoring algorithms. \n",
    "\n",
    "For this task, you will be asked to find a better similarity module to improve the search performance. Please refer [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/similarity.html) for a better understanding of the configurable elasticsearch similarity modules.\n",
    "\n",
    "Here's some code to get you started and for you to complete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: define your own analyzer for indexing and searching\n",
    "q3_settings = {\n",
    "  \"mappings\": {\n",
    "    \"doc\": {\n",
    "      \"properties\": {\n",
    "        \"filename\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"path\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"text\": {\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "        \"my_analyzer\": {\n",
    "          \"filter\": [\n",
    "            \"stop\"\n",
    "          ],\n",
    "          \"char_filter\": [\n",
    "            \"html_strip\"\n",
    "          ],\n",
    "          \"type\": \"custom\",\n",
    "          \"tokenizer\": \"whitespace\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run this block to generate an output based on q3_settings defined above.\n",
    "build_gov_index(es_conn, INDEX_NAME, DOCS_PATH, q3_settings)\n",
    "read_search_write_output(search, query_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./trec_eval/trec_eval ./gov/qrels/gov.qrels output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the final output to Wattle, but **please first rename output_file to YourUID_output_q3.txt eg u1234567_output_q3.txt - FAILURE TO DO SO WILL RESULT IN -0.5 POINTS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Improving the indexer: compare different ways of indexing (1 mark)\n",
    "\n",
    "For this part, you will be asked to change the configuration of indexer (`basic_settings`) to improve the search performance.\n",
    "\n",
    "Please look at the elastic search official document [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis.html) for better understanding of configuration and other options.\n",
    "\n",
    "Note that you can check how your tokeniser tokenises your input string via the `analyze_query` function provided in the demo code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: configure settings to define your own analyzer for indexing\n",
    "q4_settings = {\n",
    "  \"mappings\": {\n",
    "    \"doc\": {\n",
    "      \"properties\": {\n",
    "        \"filename\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"path\": {\n",
    "          \"type\": \"keyword\",\n",
    "          \"index\": False,\n",
    "        },\n",
    "        \"text\": {\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"settings\": {\n",
    "    \"analysis\": {\n",
    "      \"analyzer\": {\n",
    "          \"my_analyzer\": {\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run this block to generate an output based on q4_settings and my_search defined above.\n",
    "build_gov_index(es_conn, INDEX_NAME, DOCS_PATH, q4_settings)\n",
    "read_search_write_output(search, query_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./trec_eval/trec_eval ./gov/qrels/gov.qrels output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the final output to Wattle, but **please first rename output_file to YourUID_output_q4.txt eg u1234567_output_q3.txt - FAILURE TO DO SO WILL RESULT IN -0.5 POINTS**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Tolerant retrieval: wildcard queries (1 mark)\n",
    "\n",
    "*Elasticsearch* provides wildcard query search. You can use wildcard expressions cosisting of '*' and '?' to search.  \n",
    "\n",
    "For this task, you can reuse the previous index, i.e., *q4_settings*. Refer to the [link](https://www.elastic.co/guide/en/elasticsearch/reference/6.3/query-dsl-wildcard-query.html) to see how to search with wildcard queries. \n",
    "\n",
    "For each query term from 'gov.topics', replace last two characters with any wildcard expression. For example, the first topic from 'gov.topics' is 'mining gold silver coal'. Instead, you search 'mini&ast; go?? silv&ast; co??'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_search(query_string, es_conn, index_name):\n",
    "    res = es_conn.search(index = index_name,\n",
    "        body = {\n",
    "            \"_source\": [ \"filename\"],\n",
    "            \"query\": {\n",
    "                \"query_string\": {\n",
    "                    \"query\": query_string,\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return res['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run this block to generate the output\n",
    "read_search_write_output(my_search, query_path, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./trec_eval/trec_eval ./gov/qrels/gov.qrels output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written component (Q6 - Q9), 6 marks\n",
    "\n",
    "Answer the following questions based on your implementation of Questions 1-5:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 (1.5 marks): What changs did you make to the search similarity to improve the performance of the system? Why do you think it improved the performance?\n",
    "\n",
    "(provide answers below using bullet points with 2~3 items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT ANSWERS TO Q6 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 (1.5 marks): What changes did you make to the indexer to improve the performance of the system? Why do you think it improved the performance?\n",
    "\n",
    "(provide answer below using bullet points with 2~3 items (Check [this](https://sourceforge.net/p/jupiter/wiki/markdown_syntax/#md_ex_lists) if you are not familiar with markdown syntax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT ANSWERS TO Q7 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 (1.5 marks): Apart from Precision@10, what other metrics can be used to measure the performance of the developed IR system for the government document collection? Provide two metrics and explain why they would be suited for this particular government IR system.\n",
    "\n",
    "(provide answers below using bullet points with 2~3 items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT ANSWERS TO Q8 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 (1.5 marks): How do wildcard queries affect performance of the retrieval in terms of measures you answered for Q8? Also provide some situations when wildcard queries are useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INSERT ANSWERS TO Q9 HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Academic Misconduct Policy**: All submitted written work and code must be your own (except for any provided starter code, of course) – submitting work other than your own will lead to both a failure on the assignment and a referral of the case to the ANU academic misconduct review procedures: ANU Academic Misconduct Procedures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
